
	Intel Data Plane Development Kit (DPDK)

1. Overview
===========
Intel Data Plane Development Kit (DPDK) presents a solution for fast-path
network processing. DPDK allows you to make use of packet processing features
available on the latest Intel processors.

This README documents the use of the DPDK feature. We do not provide a
detailed description or usage details for DPDK, the Intel DPDK
documentation should be reviewed for this purpose. (http://dpdk.org/doc)

2. Build Configuration
======================

2.1 Intel BSP Specification
---------------------------
DPDK can be built for intel-x86-64 and qemux86-64 BSP.

When the DPDK feature is included in your build, either via setup or
by editing local.conf, the resulting images will be configured to
support DPDK and include the DPDK userspace applications.

The DPDK feature will make the following changes to the target:
   * Target kernel configuration updates to support PCI MSI, HugeTLB and UIO
   * Target configuration updates to mount HugeTLB
(At the time of writing DPDK *only* supports 'linuxapp', no baremetal, it is
 worth noting that the DPDK feature will always prepare the target for
 'linuxapp' support regardless if you plan on using it or not)

In order to use the DPDK feature you must include the feature and meta-dpdk
layer in your call to setup.sh

   --templates=feature/dpdk --layers=meta-dpdk

(for an existing build you can add the DPDK feature by adding the template and layer
 to the build's local.conf and bblayers.conf, respectively)

Once the DPDK feature is included you can proceed with your normal build.

Note:
DPDK requires to walk through its process' address space via /proc/self/pagemap
to get its usage of physical memory. That exposes its process' address space and
its physical memory location to a kernel exploitation. Please review your
security policies to ensure this risk is mitigated if needed.

2.2 NXP BSP Specification
---------------------------
For NXP bsp, users should follow the above configuration to make related
bsp to use this dpdk template, and users should notice that the provider of
dpdk for NXP bsps is meta-imx layer. Besides, NXP bsp will specify the dpdk
version in machine conf by default, like:
        PREFERRED_VERSION_dpdk= "21.11"

in conf/machine/<bsp-name>.conf.
Users can build the project as normal, the outcomes will include the dpdk
package in rootfs.

3. DPDK Linuxapp Setup
======================
A kernel module named vfio_pci.ko that must be inserted on the target prior to
executing DPDK processes:

	# modprobe vfio_pci

Bind Intel devices to vfio_pci:

	# ifconfig ethX down
	# /usr/bin/dpdk-devbind.py --bind=vfio-pci 07:00.0

NOTE: This test method need two intel ethernet (e1000e/igb/ixgbe) ports,
please do this step on each port.
07:00.0 is the PCI address, use lspci to get the real address.

Reserve huge pages memory:

	# mkdir -p /mnt/huge
	# mount -t hugetlbfs nodev /mnt/huge
	# echo 64 > /proc/sys/vm/nr_hugepages

Note the default number of huge pages for DPDK is 64, which may
be not enough for the tests related to memory. In this case, cat 1024
to /proc/sys/vm/nr_hugepages.

(you will most likely want to update your system configs to automate this
 to avoid the need to manually insert the module on each reboot)

The environment can be tested with:

       # /usr/bin/dpdk-test -c 0xffff -n 1

Alternatively you may run test with no options to get information on
available command line options. This will start a RTE shell which you
can then use the following commands to initiate the tests:

memory_autotest
per_lcore_autotest
spinlock_autotest
rwlock_autotest
atomic_autotest
byteorder_autotest
prefetch_autotest
cycles_autotest
logs_autotest
memzone_autotest
ring_autotest
mempool_autotest
mbuf_autotest
timer_autotest
malloc_autotest

You may choose to run 'all_autotests' in the RTE shell to run all the
autotests. Note that it may be required to increase the value of
/proc/sys/vm/nr_hugepages to complete these tests. Additionally,
eal_flags_autotest requires -m parameter with a correct value.

4. DPDK examples applications
=============================
Various DPDK example applications are built along with DPDK. To include
these in your image add the 'dpdk-example' package to your image using
IMAGE_INSTALL:append or similar in your local.conf.

4.1 DPDK verification for NXP BSP
---------------------------------
For NXP bsp, the followings show how to verify the DPDK feature.

4.1.1 Boot Arguments
--------------------
The following boot arguments can be used in order to obtain the best performance
for DPDK applications on each NXP platform:

nxp-ls1043 platform:
	default_hugepagesz=2M hugepagesz=2M hugepages=512 isolcpus=1-3 bportals=s0 qportals=s0 iommu.passthrough=1

nxp-ls1046 platform:
	default_hugepagesz=1024M hugepagesz=1024M hugepages=4 isolcpus=1-3 bportals=s0 qportals=s0 iommu.passthrough=1

nxp-ls1028 platform:
	default_hugepagesz=2M hugepagesz=2M hugepages=256 isolcpus=1 iommu.passthrough=1

4.1.2 Sample Applications
-------------------------
For NXP bsp, there are various network architectures support DPDK, such as DPAA and ENETC.
The next section will show how to test DPDK based on DPAA and ENETC.

4.1.2.1 DPDK on DPAA
--------------------
In order to verify the DPDK on DPAA, you can use the steps like the following
to run the sample applications on nxp-ls1043 and nxp-ls1046 platform with
fsl-ls1043a-rdb-usdpaa.dtb and fsl-ls1046a-rdb-usdpaa.dtb files that can be found
in tmp-glibc/deploy/images/nxp-ls1043 and tmp-glibc/deploy/images/nxp-ls1046 directory.

Take the l2fwd as example:

 <1>. Select the policy configuration file
	For ls1043 platform
	# fmc -c /usr/share/dpdk/dpaa/usdpaa_config_ls1043.xml \
		-p /usr/share/dpdk/dpaa/usdpaa_policy_hash_ipv4_1queue.xml -a
	For ls1046 platform
	# fmc -c /usr/share/dpdk/dpaa/usdpaa_config_ls1046.xml \
		-p /usr/share/dpdk/dpaa/usdpaa_policy_hash_ipv4_1queue.xml -a
 <2>. Setup hugepages for DPDK application
      This step can be ignored if hugepages are already mounted.
      Use command mount | grep hugetlbfs to check if hugepages are already setup.
	# mkdir /mnt/hugepages
	# mount -t hugetlbfs none /mnt/hugepages
 <3>. Run the layer 2 forwarding application:
	For ls1043 platform
	# /usr/share/dpdk/examples/dpdk-l2fwd -c 0x2 -n 1 -- -p 0x1 -q 1 -T 0
	For ls1046 platform
	# /usr/share/dpdk/examples/dpdk-l2fwd -c 0x2 -n 1 -- -p 0x1 -q 1 -T 0 -b 7

4.1.2.2 DPDK on ENETC
---------------------
In order to verify the DPDK on DPAA, you can use the steps like the following
to run the sample applications on nxp-ls1028 platform with fsl-ls1028a-rdb-dpdk.dtb
file that can be found in tmp-glibc/deploy/images/nxp-ls1028 directory.

Take the l2fwd as example:

 <1>. Bind Ethernet devices to "vfio-pci" driver.
	This script enables two Ethernet devices to be used by DPDK applications by binding them to "vfio_pci" driver.
	These devices on box are labeled as "1G MAC0" and "1G SWP0".
	# cd /usr/local/dpdk/enetc/
	# ./dpdk_configure_1028ardb.sh

 <2>. Run the layer 2 forwarding application:
	# /usr/share/dpdk/examples/dpdk-l2fwd -c 0x2 -n 1 -- -p 0x1 -q 1 -T 0

Note: Please deploy a PCIe NIC on board if you intent to use NFS rootfs, because all
      the network interfaces are assigned to user space applications.

5. DPDK compilation with optimization
=====================================
DPDK supports compilation with optimization for a specific
machine. For best usage, please change set the definition of
DPDK_TARGET_MACHINE in your build's local.conf
(ie. DPDK_TARGET_MACHINE:pn-dpdk = "<target>". The available machines
are native(guessing current machine), atm(Intel Atom Processor),
nhm(Intel Nehalem Processor), wsm(Intel Westmere Processor), snb(Intel
Sandy Bridge Processor) and ivb(Intel Ivy Bridge Processor).  It is
not recommended to use "native" because it will guess the host machine
via /proc/cpuinfo against the real target machine.

6. Additional documentation
===========================
Additional READMEs relating to the DPDK can be found in the dpdk/docs directory.


#@TYPE: Wrtemplate
#@NAME: dpdk
#@DESCRIPTION: Enable Data Plane Development Kit (DPDK)
