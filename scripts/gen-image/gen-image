#!/usr/bin/env python3
#
# Copyright (C) 2020 Wind River Systems, Inc.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

import os
import sys
import subprocess
import argparse
import logging
import shutil
import glob
import time
import hashlib
import git
import socket
import datetime
import tarfile

logger = logging.getLogger('gen-image')

def get_project():
    runqemu_dir = os.path.realpath(shutil.which("runqemu"))
    return os.path.realpath("%s/../../../../" % runqemu_dir)

def run_cmd(cmd, logger=logger, cwd=''):
    logger.info('Running %s' % cmd)

    try:
        if cwd:
            output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, cwd=cwd)
        else:
            output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        output = output.decode('utf-8')
        logger.debug('output: %s' % output)
    except subprocess.CalledProcessError as e:
        raise Exception("%s\n%s" % (str(e), e.output.decode('utf-8')))

    return output

def run_bitbake_cmd(target, logger):
    cmd = 'bitbake %s' % target
    output = run_cmd(cmd)
    return output

def set_logger(logger):
    logger.setLevel(logging.DEBUG)

    class ColorFormatter(logging.Formatter):
        FORMAT = ("$BOLD%(name)-s$RESET - %(levelname)s: %(message)s")

        BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE = list(range(8))

        RESET_SEQ = "\033[0m"
        COLOR_SEQ = "\033[1;%dm"
        BOLD_SEQ = "\033[1m"

        COLORS = {
            'WARNING': YELLOW,
            'INFO': GREEN,
            'DEBUG': BLUE,
            'ERROR': RED
        }

        def formatter_msg(self, msg, use_color = True):
            if use_color:
                msg = msg.replace("$RESET", self.RESET_SEQ).replace("$BOLD", self.BOLD_SEQ)
            else:
                msg = msg.replace("$RESET", "").replace("$BOLD", "")
            return msg

        def __init__(self, use_color=True):
            msg = self.formatter_msg(self.FORMAT, use_color)
            logging.Formatter.__init__(self, msg)
            self.use_color = use_color

        def format(self, record):
            levelname = record.levelname
            if self.use_color and levelname in self.COLORS:
                fore_color = 30 + self.COLORS[levelname]
                levelname_color = self.COLOR_SEQ % fore_color + levelname + self.RESET_SEQ
                record.levelname = levelname_color
            return logging.Formatter.format(self, record)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(ColorFormatter())
    logger.addHandler(ch)

def get_today():
    return time.strftime("%Y%m%d%H%M%S")

def get_sha256(f):
    with open(f, 'rb') as inf:
        data = inf.read()
    m = hashlib.sha256()
    m.update(data)
    return m.hexdigest()

set_logger(logger)

def check_and_write(outfile, line):
    with open(outfile, 'a+') as f:
        f.seek(0)
        if not line in f.readlines():
            logger.info('Writting "%s" to %s' % (line.strip(), outfile))
            f.write(line)

def apply_extra_patches_internal(repo, patches_dir):
    logger.info('Applying extra patches to %s' % repo)
    g = git.cmd.Git(repo)
    for patch in os.listdir(patches_dir):
        logger.info('Applying %s' % patch)
        patch_path = os.path.join(patches_dir, patch)
        try:
            if patch == "9999-uncommited.patch":
                g.apply(patch_path)
            else:
                g.am(patch_path)
        except Exception as esc:
            logger.error('Failed to apply %s in %s: %s' % (patch_path, repo, esc))

def get_bb_var(var, bitbake_e):
    for line in bitbake_e.split('\n'):
        if line.startswith('%s=' % var):
            return line.split('=')[1].replace('"', '')
    return ''

class GenImage(object):
    """
    * Generate the following images in order:
        - Linux SDK
        - target images
        - container image
    * world build for packages feeds
    * Save/Re-use PR database prserv.sqlite3
    * Gen sources for IP review
    """

    def __init__(self):
        self.my_dir = os.path.abspath(os.path.dirname(sys.argv[0]))
        self.data_dir = os.path.join(self.my_dir, 'data')
        self.bin_image_conf = os.path.join(self.data_dir, 'wrlinux-bin-image.conf')
        self.container_conf = os.path.join(self.data_dir, 'wrlinux-container.conf')
        self.ip_conf = os.path.join(self.data_dir, 'wrlinux-ip.conf')
        self.update_conf = os.path.join(self.data_dir, 'wrlinux-packages-update.conf')

        parser = argparse.ArgumentParser(
            description="Gen images for specified machine",
            epilog="Use %(prog)s --help to get help")
        parser.add_argument('-m', '--machine',
            default='',
            help='Specify machine')
        parser.add_argument('-r', '--ip-review',
            default=False,
            help='Only generate sources for IP review', action="store_true")
        parser.add_argument('-i', '--build-images',
            default=False,
            help='Generate binary images and repos, this is the default action. -r -i will generate both images and sources', action="store_true")
        parser.add_argument('-b', '--pr-database',
            default=os.path.join(self.data_dir, 'prserv.sqlite3'),
            help='Specify PR database to use, no database will be used if specified no', action="store")
        parser.add_argument('-o', '--outdir',
            default=None,
            help='Specify the output dir', action="store")
        parser.add_argument('-u', '--upload-url',
            default=None,
            help='Specify the url to upload the files, which uses rsync to upload the files', action="store")
        parser.add_argument('-c', '--config',
            default=None,
            help="Read configuration from file, such as DISTRO, MACHINE and layers' branches", action="store")
        parser.add_argument('--recipes',
            default='',
            help='Specify the recipes to build rather than default recipes, use comma as separator for multiple ones', action="store")
        parser.add_argument('--repo-url', '--rpm-url',
            default=None,
            help='Specify the url to download (rsync) rpm packages, these packages will be used by packagefeed-stability.bbclass, the RPM_URL is the prepfix of the url, e.g, rsync://RPM_URL/<machine>/repos/rpm/', action="store")
        parser.add_argument('--update',
            default=False,
            help='Compared with the packages downloaded from repo-url, only deploy different or newly added packages, other packages will be dropped.', action="store_true")
        parser.add_argument('-n', '--dry-run',
            default=False,
            help="Don't do the build, only go through the steps, this is useful for debug.", action="store_true")
        parser.add_argument('-p', '--product', choices=['lincd', 'lts-21', 'lts-22'], required=False,
            default='lts-22',
            help='Specify the product', action="store")
        parser.add_argument('-e', '--extra-patches',
            default=None,
            help='Apply extra-patches.tar.gz, the extra-patches.tar.gz is created by previous build', action="store")
        parser.add_argument("-d", "--debug",
            help = "Enable debug output",
            action="store_const", const=logging.DEBUG, dest="loglevel", default=logging.INFO)
        parser.add_argument("-q", "--quiet",
            help = "Hide all output except error messages",
            action="store_const", const=logging.ERROR, dest="loglevel")

        self.args = parser.parse_args()

        logger.setLevel(self.args.loglevel)

        self.bitbake_path = shutil.which('bitbake')
        self.builddir = os.getenv('BUILDDIR')
        if not (self.bitbake_path and self.builddir):
            raise Exception('bitbake command is not found, this tools should be run after source oe-init-build-env')

        self.today = get_today()

        self.project = get_project()

        self.machine = self.args.machine

        self.image_minimal = 'wrlinux-image-minimal'
        self.image_full = 'wrlinux-image-full'
        self.image = os.getenv('IMAGE', '')
        if self.image:
            logger.info('Got IMAGE %s from environment vars' % self.image)
            self.images = [self.image]
        else:
            # Default image
            self.image = self.image_full
            self.images = [self.image_minimal, self.image_full]

        self.doc_dir = os.path.join(os.path.dirname(sys.argv[0]), 'doc')

        self.local_layer = os.path.join(self.project, 'layers/local')

        self.local_conf = os.path.join(self.builddir, 'conf/local.conf')
        self.bblayers_conf = os.path.join(self.builddir, 'conf/bblayers.conf')

        if self.args.product == 'lincd':
            self.handoff_name = 'WRLinux-CD-Images'
        elif self.args.product == 'lts-21':
            self.handoff_name = 'WRLinux-lts-21-Images'
        elif self.args.product == 'lts-22':
            self.handoff_name = 'WRLinux-lts-22-Images'
        else:
            self.handoff_name = self.args.product

        if self.args.outdir:
            self.outdir = self.args.outdir
        else:
            self.outdir = os.path.join(self.builddir, 'outdir')

        self.handoff_dir = os.path.join(self.outdir, self.handoff_name)

        self.cache_dir = os.path.join(self.builddir, 'cache')
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

        self.pr_db = os.path.join(self.cache_dir, 'prserv.sqlite3')
        self.pr_dir = os.path.join(self.handoff_dir, 'prserv')

        self.logs_dir = os.path.join(self.handoff_dir, 'logs')

        self.recipes = ''
        if self.args.recipes.strip():
            self.recipes = ' '.join(self.args.recipes.split(','))

        self.ostree_repo = ""

        self.extra_patches_dir = os.path.join(self.handoff_dir, 'extra-patches')

    def read_config(self):
        logger.info('Reading configuration from %s' % self.args.config)
        distro = ""
        self.layers = {}
        with open (self.args.config, 'r') as f:
            for line in f.readlines():
                if not "=" in line:
                    continue
                line_split = line.split()
                if len(line_split) < 3 or not line_split:
                    continue
                if line_split[0] == 'DISTRO':
                    distro = line_split[2]
                elif line_split[0] == 'MACHINE':
                    self.machine = line_split[2].strip('"')
                elif line_split[0] == 'WRLINUX_VERSION':
                    self.wrlinux_version = line_split[2].strip('"')
                elif line_split[0] == 'WRLINUX_BRANCH':
                    self.wrlinux_branch = line_split[2].strip('"')
                elif ":" in line_split[2]:
                    layername = line_split[0]
                    if layername == 'local':
                        continue
                    branch_id = line_split[2]
                    id = branch_id.split(':')[1].strip('"')
                    self.layers[layername] = id

            if distro:
                with open(self.local_conf, 'a') as f:
                    f.write('DISTRO = %s\n' % distro)

        # Figure out RCPL versions
        version = '_'.join(self.wrlinux_version.split('.')[:2])
        rcpl = int(self.wrlinux_version.split('.')[-1])
        if rcpl < 10:
            rcpl = "000%s" % rcpl
        elif rcpl < 100:
            rcpl = "00%s" % rcpl
        elif rcpl < 1000:
            rcpl = "0%s" % rcpl

        if self.wrlinux_branch == "LTS":
            version = "WRLINUX_%s_LTS_RCPL%s" % (version, rcpl)
        elif self.wrlinux_branch == "Base":
            version = "WRLINUX_%s_BASE_UPDATE%s" % (version, rcpl)
        else:
            logger.warning('Failed to figure out rcpl version')
            version = ""
        logger.debug('Version: %s' % version)

        # Find all layers
        cmd = "find %s/layers -path  '*/conf/layer.conf' -type f" % self.project
        layer_confs = run_cmd(cmd).split('\n')

        def checkout_branch(candidates, commit_id):
            checkedout = False
            for c in candidates:
                logger.debug('Checking out %s to %s' % (c, commit_id))
                g = git.cmd.Git(c)
                try:
                    g.checkout(commit_id)
                    checkedout = True
                    break
                except Exception as esc:
                    logger.debug(esc)

            return checkedout

        logger.info('Checking out layers according to %s' % self.args.config)
        handled_ids = []
        for layername in self.layers:
            candidates = []
            for conf in layer_confs:
                layer_conf = '/%s/conf/layer.conf' % layername
                # It may match mutiple layers, so use candidates to save them
                if conf.endswith(layer_conf):
                    layer = os.path.dirname(os.path.dirname(conf))
                    candidates.append(layer)

            commit_id = self.layers[layername]
            if commit_id in handled_ids:
                continue
            handled_ids.append(commit_id)
            checkout_branch(candidates, commit_id)
            if not checkout_branch(candidates, commit_id):
                logger.warning('Failed to checkout %s to %s, try --debug for more info' % (layername, commit_id))
                if version:
                    logger.warning('Trying to checkout %s to %s as a workaround' % (layername, version))
                    if not checkout_branch(candidates, version):
                        logger.warning('Failed to checkout %s to %s, try --debug for more info' % (layername, version))
                    else:
                        logger.warning('Checked-out %s to %s, but this may not work' % (layername, version))

        # The gen-image dir must be up-to-date
        g = git.cmd.Git(self.my_dir)
        g.checkout('m/master', self.my_dir)

    def apply_extra_patches(self):
        """
        Apply extra patches to the product
        """
        logger.info('Unpacking %s...' % self.args.extra_patches)
        dst = "extra-patches"
        if os.path.exists(dst):
            shutil.rmtree(dst)
        os.makedirs(dst)
        cmd = 'tar --strip-components 1 -xzf %s -C %s' % (self.args.extra_patches, dst)
        run_cmd(cmd)

        for d in os.listdir(dst):
            for layer in self.bblayers.split() + ['/'.join(self.bitbake_path.split('/')[0:-2])]:
                if layer.endswith('/%s' % d):
                    apply_extra_patches_internal(layer, os.path.join(os.path.abspath('extra-patches'), d))

    def check_dry_run(self, msg):
        if self.args.dry_run:
            logger.warning(msg)
        else:
            raise Exception(msg)

    def prepare(self):
        line = 'require %s\n' % self.bin_image_conf
        check_and_write(self.local_conf, line)

        logger.info('Determining machine ...')
        bitbake_e = self.do_build('-e')
        self.machine = get_bb_var('PKGDATA_DIR', bitbake_e)
        self.machine = os.path.basename(self.machine)

        # Only write MACHINE when self.args.machine != MACHINE from bitbake_e
        if self.args.machine and self.machine != self.args.machine:
            self.machine = self.args.machine
            with open(self.local_conf, 'a+') as f:
                f.write('\nMACHINE = "%s"\n' % self.args.machine)
                # Need re-run 'bitbake -e'
                tmpdir = get_bb_var('TMPDIR', bitbake_e)
                bitbake_e = ''

        logger.info('Preparing build for %s' % self.machine)
        self.repos_dir = os.path.join(self.handoff_dir, self.machine, 'repos')

        self.machines_ws = ["intel-x86-64", "bcm-2xxx-rpi4", "nxp-imx8", "nxp-s32g"]
        self.handoff_name_ws = "%s-%s" % (self.handoff_name, self.machine)
        self.handoff_dir_ws = os.path.join(self.handoff_dir, self.handoff_name_ws)

        if self.args.pr_database and os.path.exists(self.args.pr_database):
            if not self.args.pr_database.endswith('.sqlite3'):
                raise Exception('Invalid PR databse file: %s' % self.args.pr_database)
            # Compare pr_db's sizes, use the larger one.
            from_size = os.path.getsize(self.args.pr_database)
            to_size = 0
            if os.path.exists(self.pr_db):
                to_size = os.path.getsize(self.pr_db)
            if from_size > to_size:
                logger.info('Using %s as pr database' % self.args.pr_database)
                shutil.copy(self.args.pr_database, self.pr_db)
            else:
                logger.info('Using %s as pr database' % self.pr_db)
        elif self.args.pr_database in ('no', 'No', 'NO') and os.path.exists(self.pr_db):
            logger.info('Removing %s' % self.pr_db)
            os.remove(self.pr_db)

        if self.args.build_images or self.recipes or (self.machine and not self.args.ip_review):
            self.mv_to_old(self.machine)
            self.mv_to_old(self.handoff_name_ws)

        if not os.path.exists(self.handoff_dir):
            os.makedirs(self.handoff_dir)

        self.feed_archs = []
        all_archs = []
        if not bitbake_e:
            bitbake_e = self.do_build('-e')

        self.feed_archs = get_bb_var('PACKAGE_FEED_ARCHS', bitbake_e).split()
        all_archs = get_bb_var('ALL_MULTILIB_PACKAGE_ARCHS', bitbake_e).split()
        self.tmpdir = get_bb_var('TMPDIR', bitbake_e)
        self.deploy_dir_rpm = get_bb_var('DEPLOY_DIR_RPM', bitbake_e)
        self.deploy_dir_image = get_bb_var('DEPLOY_DIR_IMAGE', bitbake_e)
        self.ostree_repo = get_bb_var('OSTREE_REPO', bitbake_e)
        self.bblayers = get_bb_var('BBLAYERS', bitbake_e)
        self.wrlinux_version = get_bb_var('WRLINUX_VERSION', bitbake_e)
        self.wrlinux_branch = get_bb_var('WRLINUX_BRANCH', bitbake_e)
        self.pkgdata_dir = get_bb_var('PKGDATA_DIR', bitbake_e)
        self.base_workdir = get_bb_var('BASE_WORKDIR', bitbake_e)

        self.supported_packages = os.path.join(self.deploy_dir_image, 'supported_packages.txt')

        if not self.feed_archs:
            if not all_archs:
                raise Exception("Failed to figure out feed_archs")
            logger.info("PACKAGE_FEED_ARCHS is not defined, set it automatically")
            for arch in all_archs:
                self.feed_archs.append(arch.replace("-", "_"))

        if not self.tmpdir:
            raise Exception("Failed to figure tmpdir!")

        self.tmpdir_bn = os.path.basename(self.tmpdir)

        if self.args.repo_url:
            self.download_repos()

    def mv_to_old(self, src_name):
        src = os.path.join(self.handoff_dir, src_name)
        old_dir = os.path.join(self.outdir, 'old')
        dst = os.path.join(old_dir, '%s.%s' % (os.path.basename(src), self.today))
        if not os.path.exists(old_dir):
            os.makedirs(old_dir)
        if os.path.exists(src):
            shutil.move(src, dst)

    def do_build(self, target=None, conf=None):
        if self.args.dry_run and target != "-e":
            logger.info("Skipping the build since --dry-run is specified")
            return ""
        if conf:
            with open(self.local_conf, 'a') as f:
                f.write(conf)

        if not target:
            target = self.image
        output = run_bitbake_cmd(target, logger)
        return output

    def do_deploy(self, subdir, files, copy_twice=False):
        logger.info('Deploying %s' % subdir)

        def do_copy(src, dst_dir):
            if not os.path.exists(dst_dir):
                os.makedirs(dst_dir)
            shutil.copy(src, dst_dir)

        # The image file copy once, ovmf and document copy twice into the
        # following dirs:
        # - container-minimal
        # - container-full
        # - target-minimal
        # - target-full
        dst_dir = os.path.join(self.handoff_dir, subdir)
        dst_dir_minimal = '%s-minimal-%s' % (dst_dir, self.machine)
        dst_dir_full = '%s-full-%s' % (dst_dir, self.machine)
        dst_dir_sdk = '%s-%s' % (dst_dir, self.machine)
        if self.image_minimal in ' '.join(files):
            copy_twice = True
        for f in files:
            if self.args.dry_run and not os.path.exists(f):
                logger.warning('Failed to find %s' % f)
                continue
            if subdir.endswith('/sdk'):
                do_copy(f, dst_dir_sdk)
            elif self.image_minimal in f:
                do_copy(f, dst_dir_minimal)
            elif self.image_full in f:
                do_copy(f, dst_dir_full)
            elif copy_twice:
                do_copy(f, dst_dir_minimal)
                do_copy(f, dst_dir_full)
            else:
                do_copy(f, dst_dir)

    def gen_sdk(self):
        logger.info('Generating sdk...')

        self.do_build('%s -cpopulate_sdk' % self.image)
        deploy_files = []

        deploy_dir_sdk = os.path.join(self.tmpdir, 'deploy/sdk')
        suffixes = ['sdk.sh', 'sdk.host.manifest', 'sdk.target.manifest']
        for f in suffixes:
            wildcard = "%s/*-%s-%s-%s" % (deploy_dir_sdk, self.machine.replace('-', '_'), self.image, f)
            files = glob.glob(wildcard)
            if len(files) != 1:
                msg = "Should be only one %s file but found %s: \n%s" % (f, len(files), '\n'.join(files))
                self.check_dry_run(msg)
            else:
                deploy_files.append(files[0])

        deploy_files.append(os.path.join(self.doc_dir, 'sdk.README.md'))
        deploy_files.append(os.path.join(self.doc_dir, 'appsdk.README.md'))

        deploy_files.append(self.get_configinfo_file())
        self.do_deploy('%s/sdk' % self.machine, deploy_files)

    def get_deploy_files(self, type='', uboot=True, images=[]):
        deploy_files = []
        if not images:
            images = self.images
        for image in images:
            prefix = '%s/%s-%s' % (self.deploy_dir_image, image, self.machine)
            if type:
                deploy_files.append('%s.%s' % (prefix, type))
            deploy_files.append('%s.manifest' % prefix)
        ovmf = os.path.join(self.deploy_dir_image, 'ovmf.qcow2')
        if os.path.exists(ovmf):
            deploy_files.append(ovmf)
        if uboot:
            if self.machine == "bcm-2xxx-rpi4":
                qemu_u_boot = os.path.join(self.deploy_dir_image, 'qemu-u-boot-bcm-2xxx-rpi4.bin')
                deploy_files.append(qemu_u_boot)
        for f in deploy_files:
            logger.debug('Deploy file: %s' % f)
            if not os.path.exists(f):
                msg = "Failed to find %s" % f
                self.check_dry_run(msg)

        return deploy_files

    def get_configinfo_file(self):
        # Save config info
        config_outdir = os.path.join(self.tmpdir, 'deploy/images/%s' % self.machine)
        if not os.path.exists(config_outdir):
            os.makedirs(config_outdir)
        config_outfile = os.path.join(config_outdir, 'config_%s' % self.machine)

        logger.info('Generating %s' % config_outfile)
        newlines = []
        logfile = '%s/log/cooker/%s/console-latest.log' % (self.tmpdir_bn, self.machine)
        msg = "Failed to find %s" % logfile
        if os.path.exists(logfile):
            with open(logfile, 'r') as f:
                for line in f.readlines():
                    line_strip = line.strip()
                    if line.startswith('Build Configuration:'):
                        newlines.append(line)
                        continue
                    if newlines and not line_strip:
                        break
                    if newlines:
                        newlines.append(line)
        else:
            self.check_dry_run(msg)
        if newlines:
            newlines.append('\n')
            # Add more build info
            if os.path.exists('/etc/issue'):
                build_host = ""
                with open('/etc/issue') as f:
                    for line in f.readlines():
                        if line.strip():
                            build_host += "BUILD_HOST\t\t= \"%s\"" % line.strip()
                if build_host:
                    newlines.append(build_host + '\n')

            cmd = 'uname -a'
            uname_a = run_cmd(cmd).strip()
            newlines.append("BUILD_HOST_UNAME_A\t = \"%s\"\n" % uname_a)

            newlines.append("BUILDDIR\t\t= \"%s\"\n" % self.builddir)

            with open(config_outfile, 'w') as f:
                f.write(''.join(newlines))
        else:
            logger.warning('Failed to find config info from %s' % logfile)

        return (config_outfile)

    def gen_ostree(self):
        logger.info('Generating target image...')

        deploy_files = set()
        is_x86 = ('intel' in self.machine and self.machine != "intel-socfpga-64") or 'x86-64' in self.machine

        for image in self.images:
            self.do_build(target=image)

            # Regenerate wic image to support disk resize
            if os.path.exists(self.ostree_repo):
                date_since_epoch = datetime.datetime.now().strftime('%s')
                cmd = "%s/layers/meta-lat/scripts/bootfs.sh -l 0 -b %s -s 0 -a 'instdate=@%s instw=60'" % (self.project, image, date_since_epoch)
                run_cmd(cmd)

            new_image = ""
            deploy_files |= set(self.get_deploy_files(uboot=not is_x86, images=[image]))
            basename = '%s-%s' % (image, self.machine)
            ustart_images = ['ustart.img.bmap']
            if self.machine == 'nxp-s32g':
                ustart_images += ['evb3.ustart.img.gz', 'rdb3.ustart.img.gz']
            elif self.machine == 'intel-socfpga-64':
                ustart_images = ['stratix10.ustart.img.bmap', 'stratix10.ustart.img.gz']
            else:
                ustart_images.append('ustart.img.gz')

            for f in ustart_images:
                if self.machine == 'nxp-s32g' or self.machine == 'intel-socfpga-64':
                    newf = os.path.join(self.builddir, '%s-%s' % (basename, f))
                else:
                    newf = os.path.join(self.builddir, '%s.%s' % (basename, f))
                msg = 'Failed to rename %s: File not found' % f
                if os.path.exists(f):
                    os.rename(f, newf)
                else:
                    self.check_dry_run(msg)
                deploy_files.add(newf)

        deploy_files.add(self.get_configinfo_file())

        readme = os.path.join(self.doc_dir, 'target_%s.README.md' %  self.machine)
        if os.path.exists(readme):
            deploy_files.add(readme)
        else:
            logger.warning('No README: %s' % readme)

        ks_readme = os.path.join(self.doc_dir, 'target_kickstart.README.md')
        if os.path.exists(ks_readme):
            deploy_files.add(ks_readme)
        else:
            logger.warning('No README: %s' % ks_readme)

        write_sd_sh = os.path.join(self.doc_dir, '%s-sd.sh' %  self.machine)
        if os.path.exists(write_sd_sh):
            deploy_files.add(write_sd_sh)

        if self.machine == "intel-x86-64":
            deploy_files.add(os.path.join(self.doc_dir, 'update-grub-console.sh'))

        self.do_deploy('%s/target' % self.machine, deploy_files)

        logger.info('Deploying %s/repos/ostree_repo...' % self.machine)
        dst_dir = os.path.join(self.repos_dir, 'ostree_repo')
        if os.path.exists(self.ostree_repo):
            shutil.copytree(self.ostree_repo, dst_dir)

    def gen_container(self):
        logger.info('Generating container image...')

        for image in self.images:
            self.do_build(target='%s -r %s' % (image, self.container_conf))

        deploy_files = self.get_deploy_files(type='tar.bz2', uboot=False)

        deploy_files.append(os.path.join(self.doc_dir, 'container.README.md'))
        deploy_files.append(self.get_configinfo_file())
        deploy_files.append(self.supported_packages)
        self.do_deploy('%s/container' % self.machine, deploy_files)

    def gen_feeds(self):
        logger.info('Generating packages feeds...')
        # The docker won't be built in a world build since it is started by virtual.
        if self.recipes:
            targets = self.recipes
        else:
            targets = 'world docker %s' % ' '.join(self.images)
        self.do_build(targets)

        # Get supported pkgs list
        if not self.recipes:
            logger.info("Generating %s..." % self.supported_packages)
            with open(self.supported_packages, 'w') as f:
                for i in os.listdir(self.pkgdata_dir):
                    f.write('%s\n' % i)

            # Add supported packages list to the tarball
            self.do_deploy('%s/target' % self.machine, [self.supported_packages], copy_twice=True)
            self.do_deploy('%s/container' % self.machine, [self.supported_packages], copy_twice=True)

        # Remove unneeded archs
        existed_archs = []
        rpm_dir = os.path.join(self.tmpdir, 'deploy/rpm')
        msg = "Failed to find %s" % rpm_dir
        if os.path.exists(rpm_dir):
            existed_archs = os.listdir(rpm_dir)
        else:
            self.check_dry_run(msg)
        for e in existed_archs:
            if e.startswith('.pkgdata.tar.bz2'):
                continue
            if not e in self.feed_archs:
                e_path = os.path.join(rpm_dir, e)
                logger.info('Removing unneeded data: %s' % e_path)
                shutil.rmtree(e_path)

        package_index = "package-index"
        if self.args.update:
            rpm_dir += "-updates"
            package_index += " -R %s" % self.update_conf
            # The updated packages are saved by packagefeed-stability.bbclass
            prediff_dir = '%s-prediff' % self.deploy_dir_rpm
            fmt = os.path.join(prediff_dir, "pkg-compare-manifest-*")
            update_manifests = glob.glob(fmt)

            # Let it go on when no updates
            if not update_manifests:
                logger.warning('Failed to find "%s", no packages updated?' % fmt)

            deploy_dir_rpm_update = '%s-updates' % self.deploy_dir_rpm
            if os.path.exists(deploy_dir_rpm_update):
                logger.info('Removing %s' % deploy_dir_rpm_update)
                shutil.rmtree(deploy_dir_rpm_update)

            # Create package arch dirs even if they are empty, otherwise,
            # "dnf update" would report 404 errors
            for arch in existed_archs:
                if not arch.startswith('.pkgdata.tar.bz2'):
                    os.makedirs(os.path.join(deploy_dir_rpm_update, arch))

            for manifest in update_manifests:
                with open(manifest) as mf:
                    for pkg in mf.readlines():
                        # Remove "\n"
                        pkg = pkg.strip()
                        dst = pkg.replace(self.deploy_dir_rpm, deploy_dir_rpm_update)
                        if os.path.exists(pkg):
                            os.link(pkg, dst)
                        else:
                            # It may be removed
                            logger.info("%s not found (ignored)" % pkg)

        self.do_build(package_index)

        if not os.path.exists(self.repos_dir):
            os.makedirs(self.repos_dir)

        logger.info('Deploying %s'% rpm_dir)
        dst_dir = os.path.join(self.repos_dir, 'rpm')
        msg = 'Failed to find %s' % rpm_dir
        if os.path.exists(rpm_dir):
            shutil.copytree(rpm_dir, dst_dir)
        else:
            self.check_dry_run(msg)

    def collect_extra_patches(self):
        """
        Collecting local patches in each layer except layers/local
        """
        logger.info('Collecting local patches...')
        checked = set()
        for layer in self.bblayers.split() + ['/'.join(self.bitbake_path.split('/')[0:-2])]:
            layer = layer.strip()
            if not layer or layer.endswith('/local'):
                continue
            g = git.cmd.Git(layer)
            try:
                toplevel = g.rev_parse('--show-toplevel')
                if toplevel in checked:
                    continue
                checked.add(toplevel)

                extra_patches_layer = os.path.join(self.extra_patches_dir, '%s-%s'% (self.wrlinux_version, self.wrlinux_branch), os.path.basename(toplevel))
                # Clean previous patches
                if os.path.exists(extra_patches_layer):
                    shutil.rmtree(extra_patches_layer)

                # Check uncommited changes
                diff = g.diff()
                if diff:
                    os.makedirs(extra_patches_layer)

                    # Check uncommited changes
                    diff_patch = os.path.join(extra_patches_layer, '9999-uncommited.patch')
                    with open(diff_patch, 'w') as f:
                        f.write(diff + '\n')

                # Check commited changes
                diff = g.diff('m/master..HEAD')
                if diff:
                    g.format_patch('m/master..HEAD', '-o', extra_patches_layer)

            except Exception as esc:
                logger.warning('%s: %s' % (layer, esc))

    def post(self):
        with open(self.local_conf, 'a') as f:
            f.write('#### Done by gen-image\n')

        # Save local.conf
        local_conf_machine = '%s.%s' % (self.local_conf, self.machine)
        shutil.copy(self.local_conf, local_conf_machine)
        logger.info('The local.conf is saved to %s' % local_conf_machine)

        # Save prserver
        self.mv_to_old('prserv')
        os.makedirs(self.pr_dir)
        shutil.copy(self.pr_db, self.pr_dir)

        # Save logs for each machine
        if not os.path.exists(self.logs_dir):
            os.makedirs(self.logs_dir)
        tarname = os.path.basename(self.tmpdir) + '.tar.gz'
        self.mv_to_old('logs/%s' % tarname)
        cmd = "find %s/work/*/*/*/temp -name 'log.*' -exec cp -al --parent {} %s \;" % (self.tmpdir_bn, self.logs_dir)
        try:
            run_cmd(cmd, cwd=self.builddir)
            cmd = "tar --numeric-owner -czf %s %s" % (tarname, self.tmpdir_bn)
            run_cmd(cmd, cwd=self.logs_dir)
            shutil.rmtree(os.path.join(self.logs_dir, self.tmpdir_bn))
        except Exception:
            msg = "Failed to run: %s" % cmd
            self.check_dry_run(msg)
        self.collect_extra_patches()

        if self.recipes:
            return

        # Generate sha256 for the following files
        # - container/*.tar.bz2
        # - target/*.img.gz ovmf.qcow2
        # - sdk/*.sh
        tgts = ("container-*/*.tar.bz2", "target-*/*.img.gz", "target-*/ovmf.qcow2", "sdk/*.sh")
        sha256files = []
        file_prefix = os.path.join(self.handoff_dir, self.machine)
        for tgt in tgts:
            fmt = os.path.join(file_prefix, tgt)
            for f in glob.glob(fmt):
                sha256files.append(f)
        outlist = []
        if sha256files:
            for sha256file in sha256files:
                basename = os.path.basename(sha256file)
                logger.info('Generating checksum for %s' % basename)
                dirname = os.path.dirname(sha256file)
                outfile = os.path.join(dirname, 'sha256sum.txt')
                with open(outfile, 'a+') as f:
                    f.write('%s *%s\n' % (get_sha256(sha256file), basename))
        else:
            logger.warning("The checksum is not generated since no files are found!")

        # Create tarballs for target minimal/full and sdk directories
        if os.path.exists(file_prefix):
            for d in os.listdir(file_prefix):
                if ('-minimal-' in d or '-full-' in d or 'sdk-' in d) and not d.startswith('container-'):
                    tarname = '%s.tar.bz2' % d
                    tarname_path = os.path.join(file_prefix, tarname)
                    cmd = 'tar --numeric-owner -cjvf %s %s' % (tarname, d)
                    logger.info('Running %s' % cmd)
                    subprocess.check_output(cmd, shell=True, cwd=file_prefix)

                    if self.machine in self.machines_ws:
                        if not os.path.exists(self.handoff_dir_ws):
                            os.makedirs(self.handoff_dir_ws)
                        shutil.copy(tarname_path, self.handoff_dir_ws)

        logger.info('Deploy files: %s and WRLinux-lts-22-Images-%s' % (os.path.join(self.handoff_dir, self.machine), self.machine))

    def ip_review(self):
        logger.info('Generating sources for IP review')
        # Remove base_workdir to save disk space
        if os.path.exists(self.base_workdir) and not self.args.dry_run:
            logger.info('Removing %s' % self.base_workdir)
            shutil.rmtree(self.base_workdir)
        logger.info('Using prefile %s' % self.ip_conf)
        # The docker won't be built in a world build since it is started with "virtual/".
        if self.recipes:
            targets = '%s -c deploy_archives' % self.recipes
        else:
            targets = "world docker --runall=deploy_archives %s" % ' '.join(self.images)
        self.do_build('%s -r %s' % (targets, self.ip_conf))

        # And sdk
        # The "bitbake wrlinux-image-full --runall=deploy_archives -cpopulate_sdk"
        # doesn't work, so we figure out the list and build them one by one
        if not self.recipes:
            self.do_build('%s -r %s -cpopulate_sdk -g' % (self.image, self.ip_conf))
            recipes = []
            with open('pn-buildlist') as f:
                for line in f.readlines():
                    line = line.strip()
                    # Only native sdk recipes are needed since others have already been built
                    if line.startswith('nativesdk-') and line != 'nativesdk-libgcc-initial':
                        recipes.append(line)

            self.do_build('%s -r %s  -cdeploy_archives' % (' '.join(recipes), self.ip_conf))

        src_dir = os.path.join(self.tmpdir, 'deploy/sources')
        logger.info('Deploying sources-%s' % self.machine)
        dst_dir = os.path.join(self.handoff_dir, 'sources-%s' % self.machine)
        self.mv_to_old('sources-%s' % self.machine)
        os.makedirs(dst_dir)
        # Use hardlinks
        try:
            cmd = 'cp -al %s/*/* %s' % (src_dir, dst_dir)
            run_cmd(cmd)
            # Remove natives
            cmd = 'rm -fr %s/*-native-*' % dst_dir
            run_cmd(cmd)
        except Exception:
            msg = "Failed to run: %s" % cmd
            self.check_dry_run(msg)

        # Archive the sources
        if os.path.exists(dst_dir):
            logger.info('Creating tarballs in %s...' % dst_dir)
            if shutil.which('pigz'):
                logger.info('Using pigz...')
                tar_opt = '-I pigz -cf'
            else:
                tar_opt = '-czf'
            sources_list = os.path.join(dst_dir, 'sources-list-%s.txt' % self.machine)
            sf = open(sources_list, 'w')
            for i in os.listdir(dst_dir):
                sumfiles = []
                # Generate checksums
                topdir = '%s/%s' % (dst_dir, i)
                if not os.path.isdir(topdir):
                    continue
                for root, dirs, files in os.walk(topdir):
                    for f in files:
                        f_path = '%s/%s' % (root, f)
                        sumfiles.append(f_path.replace(topdir + '/', ''))

                sumfiles.sort()
                # Save files' sha256 to outfile, then use outfile's sha256 as
                # the tarball's suffix, the outfile is mainly used for debugging.
                outfile = '%s/gen-image-sha256sum.txt' % topdir
                with open(outfile, 'w') as f:
                    for s in sumfiles:
                        sha256 = ""
                        # Use the HEAD commit's ID as sha256 for git repo
                        s_path = '%s/%s' % (topdir, s)
                        if os.path.isfile(s_path) and tarfile.is_tarfile(s_path):
                            cmd = "tar --list --wildcards '*/*/.git/HEAD' -f %s" % s_path
                            retval, output = subprocess.getstatusoutput(cmd)
                            if retval == 0:
                                # Only extract the "./git" dir
                                cmd = "tar -x --wildcards '*/*/.git/' -f %s" % s_path
                                subprocess.check_call(cmd, shell=True, cwd=topdir)
                                g = git.cmd.Git('%s/%s' % (topdir, os.path.dirname(output)))
                                sha256 = "%s(git) *%s" % (g.log('-1', '--format=%H', 'HEAD'), s)
                                extracted_dir = output.split('/')[0]
                                if extracted_dir:
                                    shutil.rmtree('%s/%s' % (topdir, extracted_dir))
                                else:
                                    logger.warning('Failed to figure out extracted_dir: %s' % output)
                            elif s.startswith(i):
                                # The archiver.bbclass creates a tarball for 'dir'
                                # sources with a random directory name, which makes
                                # the checksum mistmatch, so save checksum of files in
                                # the dir rather than the tarball itself to fix the
                                # checksum issues.
                                extracted_dir = ""
                                cmd = "tar --list --wildcards 'tmp*/' -f %s" % s_path
                                retval, output = subprocess.getstatusoutput(cmd)
                                if retval == 0:
                                    extracted_dir = output.split('/')[0]
                                if "-recipe.tar." in s:
                                    extracted_dir = '-'.join(s.split('-')[:-1]) + "-recipe"
                                # Also extract foo-recipe.tar.xz to save the
                                # files' checksum since different foo-recipe.tar.xz
                                # have different checksums because of the creation time.
                                if extracted_dir:
                                    cmd = "tar -xf %s" % s_path
                                    subprocess.check_call(cmd, shell=True, cwd=topdir)
                                    extracted_dir_path = '%s/%s' % (topdir, extracted_dir)
                                    for root, dirs, files in os.walk(extracted_dir_path):
                                        for file in files:
                                            f_path = '%s/%s' % (root, file)
                                            if sha256:
                                                sha256 += "\n%s *%s/%s" % (get_sha256(f_path), s, file)
                                            else:
                                                sha256 = "%s *%s/%s" % (get_sha256(f_path), s, file)
                                    shutil.rmtree(extracted_dir_path)

                        if not sha256:
                            sha256 = "%s *%s" % (get_sha256(s_path), s)
                        f.write('%s\n' % (sha256))

                suffix = get_sha256(outfile)[:7]
                tarball = '%s_%s.tar.gz' % (i, suffix)
                cmd = 'tar --numeric-owner --remove-files %s %s %s' % (tar_opt, tarball, i)
                subprocess.check_call(cmd, shell=True, cwd=dst_dir)
                sf.write('%s\n' % tarball)
            sf.close()

    def do_upload(self):
        logger.info('Preparing upload files...')
        self.local_upload_dir = os.path.join(self.outdir, 'upload')
        date = time.strftime("%Y-%m-%d")
        self.local_upload_dir_date = os.path.join(self.local_upload_dir, date)
        if os.path.exists(self.local_upload_dir):
            self.mv_to_old('../upload')

        os.makedirs(self.local_upload_dir)

        local_handoff_dir = os.path.join(self.local_upload_dir_date, self.handoff_name)
        if not os.path.exists(local_handoff_dir):
            os.makedirs(local_handoff_dir)

        dst_dir_container = os.path.join(self.local_upload_dir_date, 'containers')
        dst_dir_ws = os.path.join(self.local_upload_dir_date, 'ws')
        dst_dir_readmes = os.path.join(self.local_upload_dir_date, 'READMEs')
        os.makedirs(dst_dir_container)
        os.makedirs(dst_dir_ws)
        os.makedirs(dst_dir_readmes)
        local_sources = os.path.join(local_handoff_dir, 'sources')
        os.makedirs(local_sources)
        for m in os.listdir(self.handoff_dir):
            if m in ('prserv', 'logs', "extra-patches"):
                continue
            file_prefix = os.path.join(self.handoff_dir, m)
            if os.path.exists(file_prefix):
                if m.startswith('sources-'):
                    cmd = 'cp -aln %s/* %s' % (file_prefix, local_sources)
                    run_cmd(cmd)
                    continue

                wildcard = "%s/*/*.README.md" % file_prefix
                files = glob.glob(wildcard)
                if len(files) >= 1:
                    cmd = 'cp -aln %s %s' % (wildcard, dst_dir_readmes)
                    run_cmd(cmd)

                if m.startswith('%s-' % self.handoff_name):
                    cmd = 'cp -alf %s %s' % (file_prefix, dst_dir_ws)
                    run_cmd(cmd)
                    continue

                dst_dir_tgt = os.path.join(local_handoff_dir, m)
                if not os.path.exists(dst_dir_tgt):
                    os.makedirs(dst_dir_tgt)
                for d in os.listdir(file_prefix):
                    d_path = os.path.join(file_prefix, d)
                    cmd = ""
                    if d.startswith('lat-'):
                        continue
                    elif d.startswith('container-'):
                        cmd = 'cp -alf %s %s' % (d_path, dst_dir_container)
                    elif d.endswith('.tar.bz2'):
                        cmd = 'cp -alf %s %s' % (d_path, dst_dir_tgt)
                    elif d == 'repos':
                        repos_path = os.path.join(dst_dir_tgt, d)
                        if not os.path.exists(repos_path):
                            os.makedirs(repos_path)
                        cmd = 'cp -alf %s/* %s' % (d_path, repos_path)

                    if cmd:
                        run_cmd(cmd)

        if os.path.exists(self.pr_dir):
            shutil.copytree(self.pr_dir, os.path.join(local_handoff_dir, 'prserv'))

        dst_dir_logs = os.path.join(self.local_upload_dir_date, 'logs')
        if os.path.exists(self.logs_dir):
            shutil.copytree(self.logs_dir, dst_dir_logs)

        # Create a tarball for all the extra patches
        if os.path.exists(self.extra_patches_dir):
            dst_dir_extra_patches = os.path.join(local_handoff_dir, 'extra-patches')
            for d in os.listdir(self.extra_patches_dir):
                if not os.path.exists(dst_dir_extra_patches):
                    os.makedirs(dst_dir_extra_patches)
                tarname_path = '%s/extra-patches-%s.tar.gz' % (dst_dir_extra_patches, d)
                cmd = 'tar --numeric-owner -czf %s %s' % (tarname_path, d)
                run_cmd(cmd, cwd=self.extra_patches_dir)

        # Create a symlink to latest dir
        os.symlink(date, os.path.join(self.local_upload_dir, 'latest'))

        logger.info('Done. Files are prepared in %s' % self.local_upload_dir)

        # Run rsync to remote url. Note, the slash '/' in cmd is very
        # important, otherwise, rsync would sync upload itself, but not the
        # files inside it.
        cmd = 'rsync -a %s/ %s' % (self.local_upload_dir, self.args.upload_url)
        logger.info('Running %s' % cmd)
        subprocess.check_call(cmd, shell=True)

    def is_ci_branch(self):
        try:
            g = git.cmd.Git(self.data_dir)
            head_name = g.rev_parse('--symbolic-full-name', 'm/master')
            if head_name.endswith('/WRLINUX_CI'):
                return True
        except Exception as e:
            logger.error('Failed to check ci branch: %s' % e)

        return False

    def download_repos(self):
        logger.info('Downloading rpm packages ...')
        if not os.path.exists(self.deploy_dir_rpm):
            os.makedirs(self.deploy_dir_rpm)
        cmd = 'rsync -a --delete %s/%s/repos/rpm/ %s' % (self.args.repo_url, self.machine, self.deploy_dir_rpm)
        run_cmd(cmd)

        logger.info('Downloading ostree repo...')
        if not os.path.exists(self.ostree_repo):
            os.makedirs(self.ostree_repo)
        cmd = 'rsync -a --delete %s/%s/repos/ostree_repo/ %s' % (self.args.repo_url, self.machine, self.ostree_repo)
        run_cmd(cmd)

    def gen_images(self):
        if not self.recipes:
            self.gen_ostree()
            self.gen_container()
            self.gen_sdk()
        self.gen_feeds()
        self.post()

def main():
    gen = GenImage()

    if gen.args.extra_patches:
        gen.prepare()
        gen.apply_extra_patches()
    elif gen.args.upload_url:
        gen.do_upload()
    elif gen.args.config:
        gen.read_config()
    else:
        gen.prepare()
        # Generate both images and sources
        if gen.args.ip_review and (gen.args.build_images or gen.args.recipes):
            gen.gen_images()
            gen.ip_review()
        # Generate sources only
        elif gen.args.ip_review:
            gen.ip_review()
        # Generate binary images by default
        else:
            gen.gen_images()

if __name__ == "__main__":
    try:
        ret = main()
    except Exception as esc:
        ret = 1
        import traceback
        traceback.print_exc()
    sys.exit(ret)
